# Traffic Gesture Dataset 설명 및 분석

## 1. 제스처 라벨 종류
Traffic Gesture Dataset은 총 8개의 제스처 라벨로 구성되어 있습니다:

- Gesture 0: **Fly**
- Gesture 1: **Come Closer**
- Gesture 2: **Slow Down**
- Gesture 3: **Wave**
- Gesture 4: **Push Away**
- Gesture 5: **Wave Through**
- Gesture 6: **Stop**
- Gesture 7: **Thank you**

이 데이터셋은 H5 형식으로 제공되며, 각 비디오에서 제스처가 발생하는 구간에 대해 라벨링이 되어 있습니다. 다만, **움직임 데이터(Motion data)**는 현재 처리 중이며, 이후 제공될 예정입니다.

## 2. OoD & Continuous Classification Datasets 설명
Traffic Gesture Dataset에는 다음과 같은 추가적인 **연속 측정 데이터(Continuous Measurements)**도 포함되어 있습니다:

### 데이터 유형:
- **only gestures**: 제스처만 포함된 구간.
- **only non-gestures**: 제스처가 아닌 동작만 포함된 구간.
- **gestures and non-gestures**: 제스처와 비제스처가 혼합된 구간.

### 추가 데이터 정보:
- **cont_measurements** 폴더에는 다음과 같은 데이터가 포함됩니다:
  - **RDMs (Range-Doppler Maps)**: 거리-도플러 맵을 통해 물체의 거리와 속도를 측정한 데이터.
  - **Target lists**: 각 프레임에서 감지된 타겟(물체)에 대한 거리, 속도, 방향 등의 정보를 담은 리스트.
  - **Spectrograms**: 시간에 따른 신호의 주파수 변화를 시각적으로 표현한 데이터.

### 데이터 구분:
- **backgrnd**: 교통 제스처에 속하지 않는 동작을 포함한 데이터.
- **cont**: 지속적으로 교통 제스처만 수행하는 구간.
- **contIdle**: 지속적인 제스처 중간에 휴식(Idle) 상태가 포함된 구간.
- **walk**: 제스처 없이 걷기 동작만 포함된 구간.

### 라벨링:
- 각 프레임에 **라벨이 속성으로** 기록되어 있으며, 다음과 같이 구분됩니다:
  - **교통 제스처 라벨**: 0~7 (각 제스처에 해당)
  - **Idle** 상태는 **라벨 10**으로 기록.
  - 제스처와 다른 동작(배경 동작)은 **라벨 11**로 기록.
  - **걷기 동작(Walk)**은 **라벨 12**로 기록.

이 데이터는 **연속 분류(continuous classification)**와 **Out-of-Distribution (OoD) 탐지**를 테스트하는 데 사용할 수 있습니다.

## 3. RDMs, Target lists, Spectrograms 설명
### RDMs (Range-Doppler Maps):
- **거리-도플러 맵**은 물체의 **거리**와 **속도** 정보를 맵 형태로 표현한 데이터입니다. 주로 **밀리미터파 레이더(mmWave Radar)** 센서를 사용하여 생성됩니다.
- **밀리미터파 레이더**는 도플러 효과를 이용해 물체의 거리와 움직임 속도를 정밀하게 측정합니다.

### Target lists:
- **타겟 리스트**는 레이더 센서로 측정된 여러 **타겟(목표물)**에 대한 정보를 나열한 데이터입니다. 각 타겟의 **거리, 속도, 방향** 등의 정보가 포함됩니다.

### Spectrograms:
- **스펙트로그램**은 시간에 따른 신호의 주파수 변화를 시각적으로 표현한 그래프입니다. **밀리미터파 레이더** 또는 **초음파 센서** 데이터를 통해 얻을 수 있으며, 움직임의 세부적인 패턴을 분석하는 데 사용됩니다.

## 4. 커스텀 모션 데이터를 위한 센서 사용
커스텀 모션 데이터를 학습시키려면 **레이더 센서**를 사용하여 **RDMs, Target lists, Spectrograms**와 같은 데이터를 기록할 수 있습니다.

### 사용할 수 있는 센서:
- **밀리미터파 레이더(mmWave Radar)** 센서:
  - **Texas Instruments (TI) AWR1642**, **IWR1443**
  - **Infineon XENSIV** 시리즈
- 이 센서들은 **RDMs** 및 **Target lists** 데이터를 제공하며, **Spectrograms**도 시간에 따른 주파수 변화를 분석하여 생성할 수 있습니다.

### 데이터 수집 방법:
1. **레이더 개발 키트** 사용:
   - TI 또는 Infineon의 레이더 개발 키트를 사용해 실시간으로 데이터를 수집하고 저장할 수 있습니다.
   
2. **레이더 SDK**를 사용하여 수집된 데이터를 **CSV** 또는 **H5 파일** 형식으로 저장할 수 있습니다.
   
3. **스펙트로그램**은 수집된 데이터를 바탕으로 **Python**의 **SciPy** 또는 **MATLAB**을 사용해 시각화할 수 있습니다.

4. **카메라 동기화**:
   - 비디오와 레이더 데이터를 동기화하여, 비디오에서 **OpenPose**나 **MMPose**를 사용해 **포즈 데이터**를 추출하고, 레이더 데이터와 결합해 학습용 데이터를 생성할 수 있습니다.

## 5. 레이더 데이터와 비디오 데이터 기반 학습 및 추론

### 1. **레이더 데이터로 학습한 경우**:
- 학습 과정에서 **RDMs, Target lists, Spectrograms**과 같은 **레이더 데이터**를 사용한 경우, 추론 시에도 **동일한 레이더 데이터**가 필요합니다.
- 레이더 데이터 기반 제스처 인식 모델은 학습할 때 사용한 데이터 형식과 일치하는 입력을 받아야 하므로, 추론 시에도 레이더 센서로부터 데이터를 받아야 합니다.
- 예를 들어, **밀리미터파 레이더 센서**를 사용하여 학습한 제스처 인식 모델은 실시간으로 레이더 데이터를 입력받아 동작을 인식합니다.

### 2. **비디오 데이터로 학습한 경우**:
- **비디오 데이터**를 사용하여 학습한 모델은 추론 시에도 **비디오 데이터만** 필요합니다.
- **포즈 인식 모델**(예: OpenPose, MMPose)을 사용해 비디오 데이터로 학습한 경우, 추론할 때 비디오 프레임만 입력으로 사용해 제스처를 인식합니다.
- 이 경우 레이더 데이터는 전혀 필요하지 않으며, 카메라로부터 제공되는 영상 데이터만으로 충분합니다.

### 3. **레이더 + 비디오 데이터를 혼합하여 학습한 경우**:
- 학습 과정에서 **레이더와 비디오 데이터를 모두 사용**했다면, 추론 시에도 **두 데이터**를 모두 사용할 수 있습니다. 
- 멀티모달 학습(예: 레이더와 비디오 데이터 결합) 모델은, 두 입력이 결합된 상태에서 학습했기 때문에 추론 시에도 레이더와 비디오 데이터를 함께 사용하거나, 둘 중 하나를 사용하도록 설정할 수 있습니다.

### 결론:
- **레이더 데이터로만 학습**한 모델은 **추론 시 레이더 데이터**가 필요하며, **비디오 데이터로만 학습**한 모델은 **비디오 데이터만**으로 추론이 가능합니다.
- 영상 데이터를 사용하여 제스처 인식을 학습하려는 경우, 학습 단계에서부터 비디오 데이터만 사용하면 **추론 시에도 레이더 센서가 필요하지 않게** 됩니다.
